# üìä –ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± —ç–º–∏—Ç–µ–Ω—Ç–∞—Ö –∏ –Ω–æ–≤–æ—Å—Ç–µ–π –æ –∫–æ–º–ø–∞–Ω–∏—è—Ö üìä
#
# –≠—Ç–æ—Ç Python-—Å–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–± –æ–±–ª–∏–≥–∞—Ü–∏—è—Ö –∏–∑ Excel-—Ñ–∞–π–ª–∞,
# –ø–æ–ª—É—á–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è —ç–º–∏—Ç–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ API –ú–æ—Å–∫–æ–≤—Å–∫–æ–π –±–∏—Ä–∂–∏, –∑–∞—Ç–µ–º –∏—â–µ—Ç
# –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ —ç—Ç–∏–º –∫–æ–º–ø–∞–Ω–∏—è–º –≤ Google News –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –≤ —Ñ–∞–π–ª—ã.
#
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º: pip install pandas requests openpyxl feedparser beautifulsoup4 emoji
#
# –ê–≤—Ç–æ—Ä: –ú–∏—Ö–∞–∏–ª –®–∞—Ä–¥–∏–Ω https://shardin.name/
# –î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: 11.02.2025
# –í–µ—Ä—Å–∏—è: 1.0
#
# –ê–∫—Ç—É–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å–∫—Ä–∏–ø—Ç–∞ –≤—Å–µ–≥–¥–∞ –∑–¥–µ—Å—å: https://github.com/empenoso/moex-bond-search-and-analysis
# 

import os
import time
import re
import requests
import pandas as pd
import feedparser
import urllib.parse
from datetime import datetime
from bs4 import BeautifulSoup
import emoji

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
import sys
sys.stdout.reconfigure(encoding='utf-8')

# üìÇ –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É Excel
EXCEL_FILE = "bonds.xlsx"


def load_excel_data():
    """üìÇ –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ Excel –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame."""
    print("üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ Excel...")
    df = pd.read_excel(EXCEL_FILE, sheet_name="–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π\n")
    return df


def fetch_company_names(df):
    """üîÑ –ü–æ–ª—É—á–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π –ø–æ —Ç–∏–∫–µ—Ä–∞–º –æ–±–ª–∏–≥–∞—Ü–∏–π."""
    company_names = []
    
    for ticker in df.iloc[:, 0]:
        url = f"https://iss.moex.com/iss/securities.json?q={ticker}&iss.meta=off"
        print(f"üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–∏–∫–µ—Ä: {ticker}")

        try:
            response = requests.get(url)
            response.raise_for_status()
            data = response.json()

            if not data["securities"]["data"]:
                print(f"‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è {ticker}\n")
                continue

            emitent_title = data["securities"]["data"][0][8]
            match = re.search(r'"([^"]+)"', emitent_title)
            company_name = match.group(1) if match else emitent_title

            company_names.append(company_name)
            print(f"‚úÖ {emitent_title} ‚Üí {company_name}\n")
        
        except (requests.RequestException, IndexError, KeyError) as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {ticker}: {e}\n")

        time.sleep(0.5)

    # üîÑ –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
    company_names = list(dict.fromkeys(company_names))
    return company_names


def create_folder():
    """üìÇ –°–æ–∑–¥–∞—ë—Ç –ø–∞–ø–∫—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π."""
    current_date = datetime.now().strftime('%Y-%m-%d')
    folder_path = f"news_{current_date}"
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)
    return folder_path


def search_news(company):
    """üîç –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∫–æ–º–ø–∞–Ω–∏–∏."""
    print(emoji.emojize(f"üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π: {company}"))
    query = urllib.parse.quote(company)
    url = f"https://news.google.com/rss/search?q={query}+when:1y&hl=ru&gl=RU&ceid=RU:ru"
    print(f"üìå –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω URL –∑–∞–ø—Ä–æ—Å–∞: {url}")
    
    feed = feedparser.parse(url)
    news_items = [
        {
            "source": entry.source.title if 'source' in entry else "Google News",
            "title": entry.title,
            "date": datetime.strptime(entry.published, "%a, %d %b %Y %H:%M:%S %Z").strftime("%Y-%m-%d %H:%M"),
            "url": entry.link
        }
        for entry in feed.entries
    ]
    
    time.sleep(3)
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(news_items)} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return news_items


def write_news_to_file(folder_path, company, news):
    """‚úçÔ∏è –ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –≤ —Ñ–∞–π–ª."""
    filename = os.path.join(folder_path, f"{company.replace(' ', '_')}.txt")
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(f"üì∞ –ù–æ–≤–æ—Å—Ç–∏ –¥–ª—è –∫–æ–º–ø–∞–Ω–∏–∏ {company}\n")
        f.write("=" * 50 + "\n\n")
        
        for item in sorted(news, key=lambda x: x['date'], reverse=True):
            f.write(f"üìÖ –î–∞—Ç–∞: {item['date']}\n")
            f.write(f"üì∞ –ò—Å—Ç–æ—á–Ω–∏–∫: {item['source']}\n")
            f.write(f"üìå –ó–∞–≥–æ–ª–æ–≤–æ–∫: {item['title']}\n")
            f.write(f"üîó URL: {item['url']}\n")
            f.write("-" * 30 + "\n\n")


def main():
    """üöÄ –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã."""
    df = load_excel_data()
    company_names = fetch_company_names(df)
    folder_path = create_folder()
    
    for company in company_names:
        news = search_news(company)
        write_news_to_file(folder_path, company, news)
        print(emoji.emojize(f"‚úçÔ∏è –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(news)} –¥–ª—è {company}\n"))
    
    print("üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print("\n–ú–∏—Ö–∞–∏–ª –®–∞—Ä–¥–∏–Ω https://shardin.name/\n")

    # –í –∫–æ–Ω—Ü–µ —Å–∫—Ä–∏–ø—Ç–∞
    input("–ù–∞–∂–º–∏—Ç–µ –ª—é–±—É—é –∫–ª–∞–≤–∏—à—É –¥–ª—è –≤—ã—Ö–æ–¥–∞...")

if __name__ == "__main__":
    main()