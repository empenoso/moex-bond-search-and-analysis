from datetime import datetime
import os
import emoji
import urllib.parse

import feedparser
from moex_bond_search_and_analysis.logger import Logger
from moex_bond_search_and_analysis.schemas import NewsItem


def google_search(company: str, log: Logger) -> list[NewsItem]:
    """ğŸ” Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ¸ÑĞº Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸."""
    log.info(emoji.emojize(f"\nğŸ” ĞŸĞ¾Ğ¸ÑĞº Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹: {company}"))
    query = urllib.parse.quote(company)
    url = f"https://news.google.com/rss/search?q={query}+when:1y&hl=ru&gl=RU&ceid=RU:ru"
    log.info(f"ğŸ“Œ Ğ¡Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ URL Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°: {url}")
    
    feed: feedparser.FeedParserDict = feedparser.parse(url)
    # TODO: ĞĞ°Ğ´Ğ¾ ĞºĞ°Ğº Ñ‚Ğ¾ Ñ‚Ğ¸Ğ¿Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ entry
    news_items = [
        NewsItem(
            source=entry.source.title if 'source' in entry else "Google News",
            title=entry.title,
            date=datetime.strptime(entry.published, "%a, %d %b %Y %H:%M:%S %Z"),
            url=entry.link
        )
        for entry in feed.entries
    ]

    log.info(f"âœ… ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(news_items)} Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹")
    return news_items


def write_to_file(folder_path: str, company: str, news: list[NewsItem]) -> None:
    """âœï¸ Ğ—Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ„Ğ°Ğ¹Ğ»."""
    filename = os.path.join(folder_path, f"{company.replace(' ', '_')}.txt")
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(f"ğŸ“° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸ {company}\n")
        f.write("=" * 50 + "\n\n")
        
        for item in sorted(news, key=lambda x: x.date, reverse=True):
            f.write(f"ğŸ“… Ğ”Ğ°Ñ‚Ğ°: {item.date.strftime('%Y-%m-%d %H:%M')}\n")
            f.write(f"ğŸ“° Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº: {item.source}\n")
            f.write(f"ğŸ“Œ Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº: {item.title}\n")
            f.write(f"ğŸ”— URL: {item.url}\n")
            f.write("-" * 30 + "\n\n")
